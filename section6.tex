\section*{Ensemble Methods}
\subsection*{Combining Regressors}
Set of estimators: $\hat{f}_1(x), \cdots, \hat{f}_B(x)$\\
simple average: $\hat{f}(x) = \frac{1}{B}\sum_{i=1}^B\hat{f}_i(x)$
$\mathrm{Bias}[\hat{f}(x)]=\frac{1}{B}\sum_{i=1}^B\mathrm{Bias}[f_i(x)]$\\
$\mathbb{V}[\hat{f}(x)]\approx\frac{\sigma^2}{B}$ if the estimators are uncorrelated.

\subsection*{Combining Classifiers}
Input: classifiers $c_1(x),\cdots,c_B(x)$\\
Infer $\hat{c}_B(x){=}\text{sgn}(\sum_{b=1}^B\alpha_b c_b(x))$\\
with weights $\{\alpha_b\}_{b=1}^B $\\
Requires diversity of the classifiers.

\subsection*{Bagging}
Train on bootstrapped subsets.
Covariance small, variance similar, bias weakly affected.
\textbf{Random Forest}
Collection of uncorr. decision trees.
Partition data space recursively. Grow the tree sufficiently deep to reduce bias. (random sample cuts to reduce bias).
Prediction with voting.

\textbf{Boosting} {(Weak to avoid overfitting)} \\
Combine uncorr. weak learners in sequence.
Coeff. of $\hat{c}_{b+1}$ depend on $\hat{c}_{b}$'s results\\
\textbf{AdaBoost} (minimizes exp. loss)\\
Init: $\mathcal{X}{=}\{(x_1,y_1),\cdots,(x_n,y_n)\}, w_i^{(1)}{=}\frac{1}{n}$\\
Fit  $\hat{c}_b(x)$ to $\mathcal{X}$ weighted by $w^{(b)}$\\
$\epsilon_b=\sum_{i=1}^nw_i^{(b)}\mathbb{I}_{\{\hat c_b(x_i)\not=y_i\}}/\sum_{i=1}^nw_i^{(b)}$\\
$\alpha_b = \mathrm{log}\frac{1-\epsilon_b}{\epsilon_b}>0$\\
$w_i^{(b+1)}=w_i^{(b)}\mathrm{exp}(\alpha_b\mathbb{I}_{\{{\hat c_b(x_i)\not=y_i}\}})$\\
return $\hat{c}_B(x){=}\mathrm{sgn}(\sum_{b{=}1}^B\alpha_b \hat c_b(x))$\\
Best approx. at log-odds ratio. \\
Like stagewise-additive modeling.

\textbf{Difference}
Boosting: identical $\mathcal{D}$, $\forall c(x)$ prediction weighted on accuracy, Bagging: varies $\mathcal{D}$, gives same importance.
\textbf{Notes}
AdaBoost gives high weight to hard-to-classify samples (maybe outliers). Bagging, if imbalanced dataset maybe $\mathcal{Z}$ missing a class. then, make the bootstrap size large enough s.t. at least one point is included.
\subsection*{Logistic Regression}
$log\frac{P(y=1|x)}{P(y=-1|x)} = \sum_{b=1}^Bc_b(x) =: F(x)$
$P(y=1|x) = \frac{exp(F(x)}{1+exp(F(x))}$

\section*{PAC learning}

\textit{Exp./Gen. err:} $\mathcal{R}(\hat{c}_n) {=} \mathbb{P}_{X,Y} (\hat{c}_n (x) {\neq} c(x))$

\textit{Emp. err.:} $\hat{\mathcal{R}}_n (\hat{c}_n) {=} \frac{1}{n} \sum_{i=1}^{n} 1\{\hat{c}_n (x_i) \neq y_i\}$

\textbf{Eff. PAC learnable:} $\mathcal{A}$ can learn a concept class $\mathcal{C}$ from $\mathcal{H}$ if, given a sufficiently large sample, it outputs a hypothesis that generalizes well with high probability.

$0 {<} \epsilon {<} \frac{1}{2},  0 {<} \delta {<} \frac{1}{2},  (X,Y) \in \mathcal{X} {\times} \{0,1\}:$
If $n \geq poly( \frac{1}{\epsilon}, \frac{1}{\delta}, dim(\mathcal{X}) )$, then
$\mathbb{P}_{X,Y} ( \mathcal{R}(\hat{c}_n) {-} \inf_{c \in \mathcal{C}} \mathcal{R}(c) {\leq} \epsilon ) {\geq} 1 {-} \delta.$


\subsection*{VC Inequality}
Select ERM. Under uniform convergence:
$\mathbb{P} \left( \mathcal{R}(\hat{c}_m^*) - \inf_{c \in \mathcal{C}} \mathcal{R}(c) > \epsilon \right) \leq \mathbb{P} \left( \sup_{c \in \mathcal{C}} |\hat{\mathcal{R}}_n (c) - \mathcal{R}(c)| > \frac{\epsilon}{2} \right)$:

$P(\sup |\dots| > \epsilon) \leq 2 |\mathcal{C}| \exp(-2n\epsilon^2)$
$P(\sup |\dots| > \epsilon) \leq 9 n^{V_{\mathcal{C}}} \exp \left( - \frac{n \epsilon^2}{32} \right)$


$\mathbb{P}[\mathcal{R}(\hat{c})-\inf_{c\in\mathcal{C}}\mathcal{R}(c)>\epsilon]<1-\delta$.\\
Def R.H.S.${\leq}\delta$: $\epsilon{=}\sqrt{\frac{\log N - \log(\delta/2)}{2n}}$. \\
Consider $\mathcal{H}_\epsilon {=} \{ h {\in} \mathcal{H} : R(h) {>} \epsilon \}$. 
We bound the probability of bad learning for consistent learn.: 
$P(\exists h \in \mathcal{H}_\epsilon : \hat{R}(h) = 0) \leq \sum_{h \in \mathcal{H}_\epsilon} P(\hat{R}(h) = 0) $
$\leq |\mathcal{H}_\epsilon| (1 - \epsilon)^m \leq |\mathcal{H}| \exp(-m\epsilon) \leq \delta$
$\Rightarrow m \geq \frac{1}{\epsilon} \left( \log(|\mathcal{H}|) + \log \left( \frac{1}{\delta} \right) \right)$




% \subsection*{Hyperplane learning}
% Hypothesis: $\sum_{i=1}^d a_ix_i + a_0$ with \#-classifiers $2\binom{n}{d}$. the classifiers $c$ and $\hat{c}$ differ for no more than $d$ data points on a plane, IF found with ERM: $\forall_{c\in\mathcal{C}} \hat{\mathcal{R}}_n(c) \geq \hat{\mathcal{R}}_n(\hat{c}) - \frac{d}{n}$.
\subsection*{VC dimension}
classifier can shatter any $n$ but no some $n{+}1$ points.
 \textbf{Examples:}
$(-\infty, a] =1$ 
all intervals in R: $V_C{=}2$
For k intervals, $2k$
half planes in $R^2$: $3$
for unit circles $3$
convex polygons in $R^2$: $\infty$
convex polygons in $R^2$ with at most k vertices: $2k+1$

\section*{Nonparametric Bayesian methods}
$\text{Beta}(x|a,b)=B(a,b)^{-1} x^{a-1}(1-x)^{b-1}$: prob. of Bernoulli proc. after observing $a-1$ success and $b-1$ failures. Multivariate case: Dirichlet distr. that will give multivar. probs, \textit{based on finite} counts! But we don't know exactly which multivar. distribution works. With more data, we update the Dirichlet distribution. Is a conjugate prior.\\
\textbf{Stick-breaking Dirichl. proc.} \\ Repeatedly draw from $\text{Beta}(x|1,\alpha)$ with fixed $\alpha$, but from reducing stick: $\rho_k=\beta_k(1-\sum_{i=1}^{k-1}\rho_i)$. The prior:\\
$\mathbb{P}[z_i=k|z_{-i},\alpha]=\begin{cases}\frac{N_{k,-i}}{\alpha+N-1} & \text{existing }k \\ \frac{\alpha}{\alpha+N-1} & \text{otherwise}\end{cases}$ \\
Final Gibbs sampler:\\
$\mathbb{P}[z_i=k|z_{-i},\alpha,\mu]=\begin{cases}\frac{N_{k,-i}}{\alpha+N-1}p(x_i|x_{-i,k},\mu) & \text{existing }k \\ \frac{\alpha}{\alpha+N-1}p(x_i,\mu) & \text{otherwise}\end{cases}$

\subsection*{Gibbs sampling}
Init: assign all data to a cluster, with prior $\pi_i$, with $\sum_{k=1}^K\pi_i<1$ (s.t. new clusters possible). E.g. with stick-breaking. \\
Then remove $x$ from $k$ and compute new $\theta_k$, then compute Gibbs sampler prob. (CRP), and sample the new cluster assignment $z_i\sim p(z_i|x_{-i},\theta_k)$. If cluster is empty, \\
remove it and decrease $K$.