\section*{Classification}
% group points in classes $1,\cdots, k, \mathcal{D}, \mathcal{O}$\\
% $\mathcal{D}$: doubt class, $\mathcal{O}$: outliers.\\
% Data: $\mathcal{Z}=\{z_i=(x_i,y_i):1\leq i\leq n\}$ 
% Assume we know $p_y(x){=}P[X{=}x|Y{=}y]$\\
% Found: classifier $\hat{c}:\mathcal{X}{\rightarrow}\mathcal{Y}{:=}\{1,\cdots, \mathcal{D}\}$\\
% Error: $\hat{R}(\hat{c}|\mathcal{Z})=\sum_{(x_i,y_i)\in\mathcal{Z}}\mathbb{I}_{\{\hat{c}(x_i)\not=y_i\}}$\\
% Expected Error:\\
% $\mathcal{R}(\hat{c}) = \sum_{y\leq k}P[y]\mathbb{E}_{x|y}[\mathbb{I}_{\{\hat{c}(x_i)\not=y_i\}}|Y=y]$
% (add term from $\mathcal{D}$)


\subsection*{Definitions and Lemmas from Chapter 7}

\textbf{Definition 11 (Constrained Optimization Problem)}  
A constrained optimization problem is of the form:
\[
\min_{w \in \mathbb{R}^d} f(w) \quad \text{s.t.} \quad g_i(w) = 0, \, i \leq m, \quad h_j(w) \leq 0, \, j \leq n.
\]
A feasible solution satisfies all constraints, and an optimal solution minimizes \( f(w) \) over all feasible solutions&#8203;:contentReference[oaicite:0]{index=0}.

\textbf{Definition 12 (Convex Optimization Problem)}  
A constrained optimization problem is convex if \( f, g_1, ..., g_m, h_1, ..., h_n \) are convex and the feasible region is convex&#8203;:contentReference[oaicite:1]{index=1}.

\textbf{Definition 13 (Lagrangian)}  
The Lagrangian of a constrained optimization problem is:
\[
L(\lambda, \alpha, w) = f(w) + \sum_{i \leq m} \lambda_i g_i(w) + \sum_{j \leq n} \alpha_j h_j(w).
\]
where \( \lambda \) and \( \alpha \) are the Lagrange multipliers&#8203;:contentReference[oaicite:2]{index=2}.

\textbf{Lemma 14 (Necessary Conditions for Optimal Solutions)}  
Any optimal solution \( w^* \) must satisfy:
\[
\nabla_w L(\lambda, \alpha, w) = 0, \quad g_i(w) = 0, \quad h_j(w) \leq 0, \quad \alpha_j \geq 0.
\]
for \( i \leq m \), \( j \leq n \)&#8203;:contentReference[oaicite:3]{index=3}.

\textbf{Lemma 15 (Dual Function and Lower Bound)}  
Given the Lagrangian \( L(\lambda, \alpha, w) \), the dual function is:
\[
\theta(\lambda, \alpha) = \inf_w L(\lambda, \alpha, w),
\]
which satisfies:
\[
\max_{\lambda, \alpha \geq 0} \inf_w L(\lambda, \alpha, w) \leq f(w^*).
\]
This forms the basis of the dual problem&#8203;:contentReference[oaicite:4]{index=4}.

\textbf{Definition 17 (Strong Duality)}  
A convex optimization problem satisfies strong duality if:
\[
\theta(\lambda^*, \alpha^*) = f(w^*),
\]
where \( w^* \) solves the primal and \( (\lambda^*, \alpha^*) \) solves the dual&#8203;:contentReference[oaicite:5]{index=5}.

\textbf{Definition 18 (Slater’s Condition)}  
A convex optimization problem satisfies Slater’s condition if there exists a strictly feasible point \( w_0 \) such that:
\[
h_j(w_0) < 0, \quad \forall j \leq n.
\]
This ensures strong duality&#8203;:contentReference[oaicite:6]{index=6}.

\textbf{Lemma 19 (Slater's Condition and Strong Duality)}  
If a convex optimization problem satisfies Slater’s condition, then strong duality holds&#8203;:contentReference[oaicite:7]{index=7}.

\textbf{Lemma 20 (Complementary Slackness)}  
If strong duality holds, then for any optimal solution \( w^* \):
\[
f(w^*) = L(\lambda^*, \alpha^*, w^*),
\]
\[
\alpha_j h_j(w^*) = 0, \quad \forall j \leq n.
\]
which enforces that inactive constraints have zero dual multipliers&#8203;:contentReference[oaicite:8]{index=8}.


\subsection*{Bayes Optimal Classifier}
Minimizes total risk for 0-1 Loss\\
$\hat{c}(x){=}
\begin{cases} 
       y & \mathbb{P}[y|x]>{1-d},\exists y\\
       \mathcal{D} & \mathbb{P}[y|x]<1-d,\forall y
   \end{cases}
 $\\
%  Generalize to other loss functions

%  \subsection*{Discriminant Functions}
%  Functions $g_k(x)\quad1\leq k\leq K$\\
%  Decide: $g_y(x){>}g_z(x)\forall z \not{=} y {\Rightarrow}$ chose $y$\\
%  Constant factor doesn't change decision.\\
%  $g_k(x)=P[y|x]\propto P[x|y]P[y] \Rightarrow$\\
%  $g_k(x){=}lnP[x|y]+lnP[y]{=}lnP[x|y]+\pi_y$
%  implements an opt. Bayes classifier.

% \subsection*{Decision Surface of Discriminant}
% Solve: $g_{k_1}(x)-g_{k_2}(x)=0$
% Special case with Gaussian classes:\\
% if $\Sigma_y = \Sigma \Rightarrow$ linear decision surface
% $g_k(x){=}w^T(x-x_0)\quad w{=}\Sigma^{-1}(\mu_1{-}\mu_2)$\\
% $x_0{=}\frac{1}{2}(\mu_1{+}\mu_2){-}\frac{\sigma^2(\mu_1-\mu_2)}{(\mu_1-\mu_2)^T\Sigma^{-1}(\mu_1-\mu_2)}\mathrm{log}\frac{\pi_1}{\pi_2}$

\subsection*{Linear Classifier}
% optimal for Gaussian with equal cov. Stat. simplicity \& comput. efficiency.
$g(x)=a^T\tilde{x}\quad a=(w_0,w)^T, \tilde{x}=(1,x)^T$\\
$a^T\tilde{x}_i>0 \Rightarrow y_i=1, a^T\tilde{x}_i<0 \Rightarrow y_i=2$\\
Normalization: $\tilde{x}_i\rightarrow-\tilde{x}_i$ if $y_i=2$ \\
Find $a$: $a^T\tilde{x}_i>0,\forall_i$!
\subsection*{Perceptron Criterion}
$J_P(a)=\sum_{\tilde{x}\in\mathcal{X}^\text{msc}}(-a^T\tilde{x})$,\\
$\Rightarrow a_{k+1}=a_k+\eta_k \sum_{\tilde{x}\in\mathcal{X}^\text{miscl}} class* \tilde{x}$\\
Converges if data separable.
\subsection*{WINNOW Algorithm}
Performs better when many dimensions are irrelevant. Search for 2 weight vectors $a^+, a^-$ (for each class). If a point is misclassified:
$a_i^+ {\leftarrow} \alpha^{+\tilde{x}_i}a_i^+, a_i^- {\leftarrow} \alpha^{-\tilde{x}_i}a_i^-$ (class 1 err.)
$a_i^+ {\leftarrow} \alpha^{-\tilde{x}_i}a_i^+, a_i^- {\leftarrow} \alpha^{+\tilde{x}_i}a_i^-$ (class 2 err.)
Exponential update.

%\subsection*{Fisher's Linear Discriminant Analysis}
%Maximize distance of the means of the projected classes to find projection plane separating them best.\\
%proj mean: $\tilde{\mu}_{\alpha}{=}\frac{1}{n_{\alpha}}\sum_{x\in\mathcal{X}_{\alpha}}w^Tx{=}w^T\mu_{\alpha}$\\
%Dist of proj means: $|w^T(\mu_1-\mu_2)|$
%Classes proj. cov: $\tilde{\Sigma}_1{+}\tilde{\Sigma}_2{=}w^T(\Sigma_1{+}\Sigma_2)w$\\
%5Fishers Criterion:\\
%$J(w)=\frac{||\mu_1-\mu_2||^2}{\tilde{\Sigma}_1{+}\tilde{\Sigma}_2}=\frac{w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw}{w^T(\Sigma_1{+}\Sigma_2)w}$
%Fishers Crit for Multiple Classes:\\
%$J(W)=\frac{|W^TS_BW|}{W^TS_WW}$\\
%$S_B=\sum_{i=1}^kn_k(\mu_k-\mu)(\mu_k-\mu)^T$\\
%$S_W=\sum_{i=1}^k\sum_{x\in \mathcal{D}_i}(x-\mu_i)(x-\mu_i)^T$

%\textbf{LDA for Multiclasses}: 
%Reformulate as $(k-1)$ ``class $\alpha$ - not class $\alpha$'' dichotomies. But some area are ambiguous

\subsection*{Support Vector Machine (SVM)}
Generalize Perceptron with margin and kernel.
Find plane that max. margin $m$ s.t.:\\
$z_ig(\mathbf{y})=z_i(\mathbf{w}^T\mathbf{y}+w_0)\geq m,\forall \mathbf{y}_i \in \mathcal{Y}$\\
$z_i \in \{-1,+1\}\quad \mathbf{y_i} = \phi(\mathbf{x_i})$\\
Vectors $\mathbf{y}_i$ are the support vectors \\ \\
\textbf{Functional Margin Problem}:\\
minimizes $||\mathbf{w}||$ for $m{=}1$: 
$L(\mathbf{w}, w_0, \mathbf{\alpha}) {=}$\\
$=\frac{1}{2}\mathbf{w}^T\mathbf{w}{-}\sum_{i=1}^n\alpha_i[z_i(\mathbf{w}^T\mathbf{y}_i{}+w_0){-}1]$\\
where $\alpha$s are Lagrange multipliers.\\
$\frac{\partial L}{\partial w} {=} 0$ and $\frac{\partial L}{\partial w_0} {=} 0$ give us constraints\\
$\mathbf{w}=\sum_{i=1}^n\alpha_iz_i\mathbf{y_i} \quad 0=\sum_{i=1}^n\alpha_iz_i$\\
Replacing these in $L$ we get (max $\alpha)$\\
$\tilde{L}(\mathbf{\alpha}){=}\sum_{i=1}^n\alpha_i{-}\frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_jz_iz_j\mathbf{y_i}^T\mathbf{y_j}$
with $\alpha_i\geq0\quad\mathrm{and}\quad\sum_{i=1}^n\alpha_iz_i=0$\\
This is the dual representation.
The optimal hyperplane is given by\\
$\mathbf{w^*}=\sum_{i=1}^n\alpha_i^*z_i\mathbf{y_i}$\\
$ w_0^*{=}{-}\frac{1}{2}(\mathrm{min}_{z_i=1}\mathbf{w^*}^T\mathbf{y_i}{+}\mathrm{max}_{z_i=-1}\mathbf{w^*}^T\mathbf{y_i})$\\
where $\mathbf{\alpha}$ maximize the dual problem.\\
Only Support Vectors ($\alpha_i\not=0$) contribute to the evaluation.\\
Optimal Margin: $\mathbf{w}^T\mathbf{w}=\sum_{i\in SV}\alpha_i^*$\\
Discrim.: $g^*(\mathbf{x}){=}\sum_{i\in SV}z_i\alpha_i\mathbf{y_i}^T\mathbf{y_i}{+}w^*_0$\\
$\mathrm{class} = \mathrm{sign(\mathbf{y}^T\mathbf{w}^*+w_0^*)}$ \\
\textbf{Kuhn-Tucker Conditions:} only then strong duality holds: $\quad \alpha_i^* \geq 0$\\
$\alpha_i^*(z_ig^*(y_i)-1)= 0, (z_ig^*(y_i)-1) \geq 0$


\subsection*{Soft Margin SVM}
Introduce slack to relax constraints\\
minimize $\frac{1}{2}w^Tw + C\sum_{i=1}^n \xi_i$ with respect to: 
$z_i(\mathbf{w}^T\mathbf{y}_i+w_0)\geq m(1-\xi_i)$\\
Lagrangian: $L(\mathbf{w}, w_0,\mathbf{\xi}, \mathbf{\alpha}, \mathbf{\beta}) {=}\frac{1}{2}\mathbf{w}^T\mathbf{w}+C\sum_{i=1}^n\xi_i$
${-}\sum_{i=1}^n\alpha_i[z_i(\mathbf{w}^T\mathbf{y}_i{+}w_0){-}1{+}\xi_i]$\\
${-}\sum_{i=1}^n\beta_i\xi_i$\\
$C$ controls margin maximization vs. constraint violation\\
Dual Problem same as usual SVM but with supplementary constraint:
$C \geq \alpha_i \geq 0$
\textbf{Kuhn-Tucker Conditions:} 
$\alpha_i^*(z_i(w^Ty_i+w_0)-1 + \xi)= 0, \xi_i(\alpha_i-C) = 0$
\subsection*{Non-Linear SVM}
Use kernel in discriminant function: \\ $g(\mathbf{x})=\sum_{i,j=1}^n\alpha_iz_iK(\mathbf{x_i},\mathbf{x})$\\
E.g solve the XOR Problem with: \\
$K(x,y)=(1+x_1y_1+x_2y_2)^2$

\subsection*{Multiclass SVM}
$\forall$class $z\in\{1,2,\cdots,M\}$ we introduce $\mathbf{w}_z$ and define our problem: $\quad(\mathbf{w} \text{ is stacked})$\\
$min_w \frac{1}{2}w^Tw = min_{\{w_z\}_{n=1}^M} \sum_{z=1}^M w_z^Tw_z$ \\
s.t. $(\mathbf{w}_{z_i}^T\mathbf{y}_i+w_{z_i,0})-$\\
$\quad \ \ \max_{z\not=Z_i}(\mathbf{w}_z^T\mathbf{y}_i+w_{z,0})\geq 1, \forall {\mathbf{y}_i\in \mathcal{Y}}$ \\
classification: $\hat z = argmax_z (w_z^Ty + w_{z,0})$

\subsection*{Structured SVM}
Each sample \textbf{y} is assigned to a structured output label $z$\\
Output Space Representation:\\
joint feature map: $\mathbf{\psi}(z,\mathbf{y})$\\
Scoring function: $f_{\mathbf{w}}(z,\mathbf{y})=\mathbf{w}^T\mathbf{\psi(z, \mathbf{y})}$\\
Classify: $\hat{z}=h(\mathbf{y})=\argmax_{z\in\mathbb{K}}f_{\mathbf{w}(z, \mathbf{y})}$
SVM objective: \\ $w^T\mathbf{\psi}(z_i,\mathbf{y_i})-max_{z_i \neq z}w^T\mathbf{\psi}(z,\mathbf{y_i}) \geq m$ \\
with margin rescaling: $min_{w, \xi \geq 0} \frac{1}{2}w^Tw + C \sum_{i=1}^n \xi_i$ s.t. $w^T\mathbf{\psi}(z_i,\mathbf{y_i})-\Delta(z,z_i)
-w^T\mathbf{\psi}(z,\mathbf{y_i}) \geq - \xi_i$ $\forall z \neq z_i$ $\forall i$ \\
Lagrangian: let $\mathbb{K}_i = \mathbb{K} \backslash {z_i}$ \\ $\frac{1}{2}w^Tw + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \sum_{z_j \in \mathbb{K}_i} \alpha_{i,j} (w^T \psi(z_i, y_i)- \Delta (z_j, z_i) - w^T \psi(z_j,y_i)+ \xi_i) - \sum_{i=1}^n \beta_i \xi_i$ with $\alpha_{i,j} \geq 0, \beta_i \geq 0$ 
