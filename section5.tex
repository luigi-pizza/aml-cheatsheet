\section*{Convex Optimization}
% group points in classes $1,\cdots, k, \mathcal{D}, \mathcal{O}$\\
% $\mathcal{D}$: doubt class, $\mathcal{O}$: outliers.\\
% Data: $\mathcal{Z}=\{z_i=(x_i,x_i):1\leq i\leq n\}$ 
% Assume we know $p_y(x){=}P[X{=}x|Y{=}y]$\\
% Found: classifier $\hat{c}:\mathcal{X}{\rightarrow}\mathcal{Y}{:=}\{1,\cdots, \mathcal{D}\}$\\
% Error: $\hat{R}(\hat{c}|\mathcal{Z})=\sum_{(x_i,y_i)\in\mathcal{Z}}\mathbb{I}_{\{\hat{c}(x_i)\not=y_i\}}$\\
% Expected Error:\\
% $\mathcal{R}(\hat{c}) = \sum_{y\leq k}P[y]\mathbb{E}_{x|y}[\mathbb{I}_{\{\hat{c}(x_i)\not=y_i\}}|Y=y]$
% (add term from $\mathcal{D}$)

Given constrained optimization problem:
$\min_{w \in \mathbb{R}^d} f(w) :\ \ \ \  g_{1:m}(w) {=} 0,  h_{1:n}(w) {\leq} 0$ \\
it is convex if $f, g_{1:m}, h_{1:n}$ are convex and the feasible region is convex.

The Lagrangian with Lagrange multipliers $\eta = (\lambda, \alpha)$:
$L(\eta, w) = f(w) + \sum_{i \leq m} \lambda_i g_i(w) + \sum_{j \leq n} \alpha_j h_j(w)$

Any \textbf{optimal solution} $W$ satisfies:
$\nabla_w L(\eta, W) {=} 0, g_i(W) {=} 0,  h_j(W) {\leq} 0, \alpha_j {\geq} 0$

the \textbf{Dual Problem} is and satisfies $\forall w$: \\
$\max_{\alpha \geq 0,\lambda} \left[\theta(\eta) := \inf_w L(\eta, w)\right] {\leq} f(w^*)$

\textbf{strong duality} if $\theta(\eta^*) {=} f(w^*)$ \\
\textbf{Slaterâ€™s cond.} if $\exists w_0$ feasible: $h_{1:n}(w_0) {<} 0$

strong duality$\rightarrow w^\star$:
$f(w^*) {=} L(\eta^*, w^*)$ and 
$\alpha_j h_j(w^*) = 0, \quad \forall j \leq n.$


% \subsection*{Bayes Optimal Classifier}
% Minimizes total risk for 0-1 Loss\\
% $\hat{c}(x){=}
% \begin{cases} 
%        y & \mathbb{P}[y|x]>{1-d},\exists y\\
%        \mathcal{D} & \mathbb{P}[y|x]<1-d,\forall y
%    \end{cases}
%  $\\
%  Generalize to other loss functions

%  \subsection*{Discriminant Functions}
%  Functions $g_k(x)\quad1\leq k\leq K$\\
%  Decide: $g_y(x){>}g_z(x)\forall z \not{=} y {\Rightarrow}$ chose $y$\\
%  Constant factor doesn't change decision.\\
%  $g_k(x)=P[y|x]\propto P[x|y]P[y] \Rightarrow$\\
%  $g_k(x){=}lnP[x|y]+lnP[y]{=}lnP[x|y]+\pi_y$
%  implements an opt. Bayes classifier.

% \subsection*{Decision Surface of Discriminant}
% Solve: $g_{k_1}(x)-g_{k_2}(x)=0$
% Special case with Gaussian classes:\\
% if $\Sigma_y = \Sigma \Rightarrow$ linear decision surface
% $g_k(x){=}w^T(x-x_0)\quad w{=}\Sigma^{-1}(\mu_1{-}\mu_2)$\\
% $x_0{=}\frac{1}{2}(\mu_1{+}\mu_2){-}\frac{\sigma^2(\mu_1-\mu_2)}{(\mu_1-\mu_2)^T\Sigma^{-1}(\mu_1-\mu_2)}\mathrm{log}\frac{\pi_1}{\pi_2}$

% \subsection*{Linear Classifier}
% % optimal for Gaussian with equal cov. Stat. simplicity \& comput. efficiency.
% $g(x)=a^T\tilde{x}\quad a=(w_0,w)^T, \tilde{x}=(1,x)^T$\\
% $a^T\tilde{x}_i>0 \Rightarrow y_i=1, a^T\tilde{x}_i<0 \Rightarrow y_i=2$\\
% Normalization: $\tilde{x}_i\rightarrow-\tilde{x}_i$ if $y_i=2$ \\
% Find $a$: $a^T\tilde{x}_i>0,\forall_i$!
% \subsection*{Perceptron Criterion}
% $J_P(a)=\sum_{\tilde{x}\in\mathcal{X}^\text{msc}}(-a^T\tilde{x})$,\\
% $\Rightarrow a_{k+1}=a_k+\eta_k \sum_{\tilde{x}\in\mathcal{X}^\text{miscl}} class* \tilde{x}$\\
% Converges if data separable.
% \subsection*{WINNOW Algorithm}
% Performs better when many dimensions are irrelevant. Search for 2 weight vectors $a^+, a^-$ (for each class). If a point is misclassified:
% $a_i^+ {\leftarrow} \alpha^{+\tilde{x}_i}a_i^+, a_i^- {\leftarrow} \alpha^{-\tilde{x}_i}a_i^-$ (class 1 err.)
% $a_i^+ {\leftarrow} \alpha^{-\tilde{x}_i}a_i^+, a_i^- {\leftarrow} \alpha^{+\tilde{x}_i}a_i^-$ (class 2 err.)
% Exponential update.

%\subsection*{Fisher's Linear Discriminant Analysis}
%Maximize distance of the means of the projected classes to find projection plane separating them best.\\
%proj mean: $\tilde{\mu}_{\alpha}{=}\frac{1}{n_{\alpha}}\sum_{x\in\mathcal{X}_{\alpha}}w^Tx{=}w^T\mu_{\alpha}$\\
%Dist of proj means: $|w^T(\mu_1-\mu_2)|$
%Classes proj. cov: $\tilde{\Sigma}_1{+}\tilde{\Sigma}_2{=}w^T(\Sigma_1{+}\Sigma_2)w$\\
%5Fishers Criterion:\\
%$J(w)=\frac{||\mu_1-\mu_2||^2}{\tilde{\Sigma}_1{+}\tilde{\Sigma}_2}=\frac{w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw}{w^T(\Sigma_1{+}\Sigma_2)w}$
%Fishers Crit for Multiple Classes:\\
%$J(W)=\frac{|W^TS_BW|}{W^TS_WW}$\\
%$S_B=\sum_{i=1}^kn_k(\mu_k-\mu)(\mu_k-\mu)^T$\\
%$S_W=\sum_{i=1}^k\sum_{x\in \mathcal{D}_i}(x-\mu_i)(x-\mu_i)^T$

%\textbf{LDA for Multiclasses}: 
%Reformulate as $(k-1)$ ``class $\alpha$ - not class $\alpha$'' dichotomies. But some area are ambiguous

\subsection*{Support Vector Machine (SVM)}
Convex constrained optimization problem with strong duality (if linearly separable). 
$\mathbf{x}_i$ support vectors, $y_i {\in} \{-1,+1\}$.

$\min_{w, w_0| \forall i \leq n: y_i (w^\top x_i + w_0) \geq 1} \quad \frac{1}{2} \|w\|^2$

\textit{Lagrangian:}
$\mathcal{L}(w, w_0, \alpha) {=} \frac{1}{2} \|w\|^2 + \sum_{i=1}^{n} \alpha_i (1 - y_i (w^\top x_i + w_0)) \quad  \alpha_i {\geq} 0.$

\textit{KKT:} $w^* {=} \sum_{i=1}^{n} \alpha_i y_i x_i$ \quad $\sum_{i=1}^{n} \alpha_i y_i {=} 0$ \\
\textit{Dual:} $\max_{\alpha\geq 0: \sum_{i=1}^{n} \alpha_i y_i = 0} \quad L(\alpha)$ \\
$\cdot L(\alpha) {=}\sum_{1}^{n} \alpha_i {-} \frac{1}{2} \sum_{1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j x_i^\top x_j$

The optimal hyperplane is given by\\
$\mathbf{w^*}=\sum_{i=1}^n\alpha_i^*y_i\mathbf{x_i}$\\
$ w_0^*{=}{-}\frac{1}{2}(\mathrm{min}_{y_i=1}\mathbf{w^*}^T\mathbf{x_i}{+}\mathrm{max}_{y_i=-1}\mathbf{w^*}^T\mathbf{x_i})$\\

Only Support Vectors ($\alpha^*_i\not=0$) contribute.\\
Optimal Margin: $\mathbf{w}^T\mathbf{w}=\sum_{i\in SV}\alpha_i^*$\\
Discrim.: $g^*(\mathbf{x}){=}\sum_{i\in SV}y_i\alpha^*_i\mathbf{x_i}^T\mathbf{x_i}{+}w^*_0$\\
$\mathrm{class} = \mathrm{sign(\mathbf{x}^T\mathbf{w}^*+w_0^*)}$ \\

\textbf{Soft Margin SVM} \\
Introduce slack to relax constraints. $C$ controls margin maximization vs. constraint violation.

$\min_{\xi_i\geq 0,w, w_0| \forall i \leq n: y_i (w^\top x_i + w_0) \geq 1-\xi_i} \\ \quad \frac{1}{2} \|w\|^2 + C\sum_{i=1}^n \xi_i $

\textit{Lagrangian}: $L(\mathbf{w}, w_0,\mathbf{\xi}, \mathbf{\alpha}, \mathbf{\beta}) {=}\frac{1}{2}\mathbf{w}^T\mathbf{w}+C\sum_{i=1}^n\xi_i$
${-}\sum_{i=1}^n\alpha_i[z_i(\mathbf{w}^T\mathbf{y}_i{+}w_0){-}1{+}\xi_i]$\\
${-}\sum_{i=1}^n\beta_i\xi_i$\\
Dual Problem same as usual SVM but with supplementary constraint:
$C \geq \alpha_i \geq 0$ \\
\textit{KTT Conditions:} 
$\alpha_i^*(z_i(w^Ty_i{+}w_0){-}1 + \xi){=} 0, \xi_i(\alpha_i-C) = 0$ \\
You should solve $\alpha$ via quadratic optimisation.
Optimal hyperplane and classification as normal SVM.
Optimal slack:  $\xi_i^* = \max(0, 1 - y_i (w^{*T} x_i + w_0^*))$
$\xi_i^* = \mathcal{L}^{\text{hinge}}(y_i,w^{*T} x_i + w_0^*)$

\textbf{Non-Linear SVM} \\
Use kernel in discriminant function: \\ $g(\mathbf{x})=\sum_{i,j=1}^n\alpha_iz_iK(\mathbf{x_i},\mathbf{x})$\\
E.g solve the XOR Problem with: \\
$K(x,y)=(1+x_1y_1+x_2y_2)^2$

\subsection*{Multiclass SVM}
$\forall$class $y\in\{1,2,\cdots,M\}$ we introduce $\mathbf{w}_y$ and define our problem: $\quad(\mathbf{w} \text{ is v-stacked})$\\
$min_w \frac{1}{2}w^Tw = min_{\{w_y\}_{n=1}^M} \sum_{y=1}^M w_y^Tw_y$ \\
s.t. $(\mathbf{w}_{y_i}^T\mathbf{x}_i+w_{y_i,0})-$\\
$\quad \ \ \max_{y\not=y_i}(\mathbf{w}_y^T\mathbf{x}_i+w_{y,0})\geq 1, \forall {\mathbf{x}_i\in \mathcal{X}}$ \\
classification: $\hat y = argmax_y (w_y^Tx + w_{y,0})$

\subsection*{Structured SVM}
Each \textbf{x} is assigned to a structured output label $y$.
Output Space Representation:\\
joint feature map: $\mathbf{\psi}(y,\mathbf{x})$\\
Scoring function: $f_{\mathbf{w}}(y,\mathbf{x})=\mathbf{w}^T\mathbf{\psi(y, \mathbf{x})}$\\
Classify: $\hat{y}=h(\mathbf{x})=\argmax_{y\in\mathbb{K}}f_{\mathbf{w}(y, \mathbf{x})}$
SVM objective: \\ $w^T\mathbf{\psi}(y_i,\mathbf{x_i})-max_{y_i \neq y}w^T\mathbf{\psi}(y,\mathbf{x_i}) \geq m$ \\
with margin rescaling: $min_{w, \xi \geq 0} \frac{1}{2}w^Tw + C \sum_{i=1}^n \xi_i$ s.t. $w^T\mathbf{\psi}(y_i,\mathbf{x_i})-\Delta(y,y_i)
-w^T\mathbf{\psi}(y,\mathbf{x_i}) \geq - \xi_i$ $\forall y \neq y_i$ $\forall i$ \\
Lagrangian: let $\mathbb{K}_i = \mathbb{K} \backslash {y_i}$ \\ $\frac{1}{2}w^Tw + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \sum_{y_j \in \mathbb{K}_i} \alpha_{i,j} (w^T \psi(y_i, x_i)- \Delta (y_j, y_i) - w^T \psi(y_j,x_i)+ \xi_i) - \sum_{i=1}^n \beta_i \xi_i$ with $\alpha_{i,j} \geq 0, \beta_i \geq 0$ 
