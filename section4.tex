\section*{Statistical Learning and Validation}

Find $f: X \to Y$ to minimize expected risk by approximation with empirical risk.

\subsection*{K-Fold Cross Validation}
Partition data $Z$ into $K$ equally sized, disjoint subsets:\\
$\mathcal{Z}=\mathcal{Z}_1\bigcup\mathcal{Z}_2\bigcup\cdots\bigcup\mathcal{Z}_K, \mathcal{Z}_\mu \bigcap\mathcal{Z}_\nu = \emptyset $
$|\mathcal{Z}_k|\approx n\frac{K-1}{K}$ \# of training samples. Learn:
$\hat{f}^{-\nu}(x){=} \argmin_{f\in\mathcal{F}}\frac{\sum_{i\not\in\mathcal{Z}_\nu}\mathcal{L}(y_i,f(x_i))}{|\mathcal{Z}-\mathcal{Z}_{\nu}|}$\\
$\hat{R}^{cv}(\mathcal{A}) = \frac{1}{n}\sum_{i\leq n}\mathcal{L}(y_i,\hat{f}^{-\kappa(i)}(x_i))$\\
Underfits because smaller dataset.\\
\textbf{Leave-one-out:} $K=n$ (unbiased but var can be large from correlated datasets)

\subsection*{Bootstrapping}
Bootstrap samples: $\mathcal{Z}^*=\{\mathcal{Z}_1^*, \cdots\mathcal{Z}_B^*\}$, of same size as original, drawn with replacement.
The chance of a sample to have appeared in the bootstrap is:\\
$1-(1-\frac{1}{n})\stackrel{n\to\infty}{\to} 1-\frac{1}{e}\approx 0.632$. So if we compute the ERM on $\mathcal{Z}$ we could get 63\% accuracy by memorization. Over-confident (shows too small bias)!\\
\textbf{Leave-one-out/out of bucket error}: compensates by computing the ERM where no memorization was for specific sample. E.g., for classification, like cross-validation:\\
$\hat{\mathcal{R}}(\mathcal{A})=\frac{1}{B}\sum_{b=1}^B\sum_{z_i\not\in\mathcal{Z}^{*b}}\frac{\mathbb{I}_{c(x_i)\neq y_i}}{B-\lvert\mathcal{Z}^{*b}\rvert}$
$\hat{R}_{0.632} = 0.368 \hat{R}(A(Z)) + 0.632 \hat{R}_{bs}$ \\
\textbf{Wald Test: }$W = \frac{\hat{\theta} - \theta_0}{\text{s.e.}(\hat{\theta})}$

\textbf{Bayesian Neural Networks (BNN)} \\
NN: no uncertainty quantification, overconfident, adversarial examples, poor generalization for domain shifts.
BNN: Using $p(w)$ and $p(D|w)$, approx. poster. by variational infer. (min rev KL).
$\sigma {\leftarrow} \sigma {-} \alpha_t \left( \epsilon^\top \frac{\partial}{\partial w} F(w, \theta) + \frac{\partial}{\partial \sigma} F(w, \theta) \right)$

\subsection*{Information-based Transductive Lear.}
ITL selects \( x_n \) that maximizes mutual information of \( y_x = f_x + \epsilon_x \) about \( f \):
\\ $x_n = \arg\max_{x \in S} I(f_A; y_x | D_{n-1})$ \\
If \( f \sim \text{GP}(\mu, k) \), then: \\
$I(f_A; y_x | D_{n-1}) = \frac{1}{2} \log \left( \frac{\text{Var}[y_x | D_{n-1}]}{\text{Var}[y_x | f_A, D_{n-1}]} \right)$

\textbf{Safe Bayesian Optimization}

$x_n = \arg\max_{x \in \hat{S}_n= \{ x \mid u^g_n(x) \geq 0 \}} u^f_n(x)$

\textbf{Batch Active Learning | ProbCover} \\
$G {=} (X, E)$, $E {=} \{(x, x') \mid \|x {-} x'\| {\leq} \delta\}$
$L {\gets} \emptyset$
$\forall i = 1, 2, \dots, b$
$\hat{x} \gets \arg\max_{x \in X} |\{x' \mid (x, x') \in E, x' \in X\}|$
$L {\gets} L \cup \{\hat{x}\}$ \ | \ \ 
$E {\gets} E {\setminus} (\{\hat{x}\} {\times} (B_{\delta}(\hat{x}) \cap X))$



% \subsection*{Jackknife}
% Estimate of an estimator $\hat{S}_n$'s Bias.\\
% $\hat{S}^{JK}=\hat{S}_n-\mathrm{bias}^{JK}$ is JK Estimator.\\
% $\mathrm{bias}^{JK}{=}(n{-}1)(\tilde{S}_n{-}\hat{S}_n)$\\
% $\tilde{S}_n{=}\frac{1}{n}\sum_{i=1}^n\hat{S}_{n{-}1}^{-i}$ avg. LOO Estimator. \\
% Debiased est. can have big variance! This is not consistent, eg. if function is median\\
% \textbf{Example Exercise Jackknife} \\
% Setup: $X \sim \mathcal{U}[0, \theta], \hat S_n = max(X) = X_n$. \\
% 1) Expected value of estimator: \\
% 1.1) CDF $P(X_n \leq x) = (\frac{x}{\theta})^n$ \\
% 1.2) PDF: $p(x) = \frac{\delta P(X_n \leq x)}{\delta x} = n \frac{x^{n-1}}{\theta^n}$ \\
% 1.3) $\mathbb{E}(\hat S_n) = \int_0^\theta x n \frac{x^{n-1}}{\theta^n} = \frac{n}{n+1}\theta$ \\
% 2) $\hat S_{n-1}^{i-1} = X_n$ if $i \neq i^*$ else $X_{n-1}$\\
% 3) $\hat S^{JK} = X_n+\frac{n-1}{n}(X_n - X_{n-1})$ \\
% 4) Expected value of $X_{n-1}$ \\
% 4.1) $P(X_{n-1} \leq x) = (\frac{x}{\theta})^n + (\frac{x}{\theta})^{n-1}\frac{\theta-x}{\theta}n$ \\
% 4.2) $p(x) = \frac{n(n-1)}{\theta}(\frac{x}{\theta})^{n-2}(1-\frac{x}{\theta})$\\
% 4.3) $\mathbb{E}(X_{n-1})= \frac{n-1}{n+1}\theta$ \\
% 5) $\mathbb{E}(\hat S_n^{JK}) = (1-\frac{1}{n^2+n})\theta$
